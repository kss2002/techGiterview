"""
Question Generation API Router

ì§ˆë¬¸ ìƒì„± ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸
"""

from typing import Dict, List, Any, Optional
import re
import uuid
import json
from fastapi import APIRouter, HTTPException, Depends, Header
from pydantic import BaseModel
from sqlalchemy import text

from app.agents.question_generator import QuestionGenerator
from app.core.database import engine

router = APIRouter()

# ì§ˆë¬¸ ìºì‹œ (ë¶„ì„ IDë³„ë¡œ ì§ˆë¬¸ ì €ì¥)
question_cache = {}


def extract_api_keys_from_headers(
    github_token: Optional[str] = Header(None, alias="x-github-token"),
    google_api_key: Optional[str] = Header(None, alias="x-google-api-key")
) -> Dict[str, str]:
    """ìš”ì²­ í—¤ë”ì—ì„œ API í‚¤ ì¶”ì¶œ"""
    api_keys = {}
    if github_token:
        api_keys["github_token"] = github_token
    if google_api_key:
        api_keys["google_api_key"] = google_api_key
    return api_keys





class QuestionGenerationRequest(BaseModel):
    """ì§ˆë¬¸ ìƒì„± ìš”ì²­"""
    repo_url: str
    analysis_result: Optional[Dict[str, Any]] = None
    question_type: str = "technical"
    difficulty: str = "medium"
    question_count: int = 9
    force_regenerate: bool = False  # ê°•ì œ ì¬ìƒì„± ì˜µì…˜


class QuestionResponse(BaseModel):
    """ì§ˆë¬¸ ì‘ë‹µ"""
    id: str
    type: str
    question: str
    difficulty: str
    context: Optional[str] = None
    time_estimate: Optional[str] = None
    code_snippet: Optional[Dict[str, Any]] = None
    expected_answer_points: Optional[List[str]] = None
    technology: Optional[str] = None
    pattern: Optional[str] = None
    # ì„œë¸Œ ì§ˆë¬¸ ê´€ë ¨ í•„ë“œ
    parent_question_id: Optional[str] = None
    sub_question_index: Optional[int] = None
    total_sub_questions: Optional[int] = None
    is_compound_question: bool = False


class QuestionCacheData(BaseModel):
    """ì§ˆë¬¸ ìºì‹œ ë°ì´í„° êµ¬ì¡°"""
    original_questions: List[QuestionResponse]  # AI ì›ë³¸ ì§ˆë¬¸
    parsed_questions: List[QuestionResponse]   # íŒŒì‹±ëœ ê°œë³„ ì§ˆë¬¸
    question_groups: Dict[str, List[str]]      # ê·¸ë£¹ë³„ ì§ˆë¬¸ ê´€ê³„ (parent_id -> [sub_question_ids])
    created_at: str


def create_question_groups(questions: List[QuestionResponse]) -> Dict[str, List[str]]:
    """ì§ˆë¬¸ ê·¸ë£¹ ê´€ê³„ ìƒì„±"""
    groups = {}
    
    for question in questions:
        if question.parent_question_id:
            parent_id = question.parent_question_id
            if parent_id not in groups:
                groups[parent_id] = []
            groups[parent_id].append(question.id)
    
    return groups


def is_header_or_title(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ ì œëª©ì´ë‚˜ í—¤ë”ì¸ì§€ í™•ì¸
    """
    text = text.strip()
    
    # 1. ë§ˆí¬ë‹¤ìš´ í—¤ë” íŒ¨í„´ (#, ##, ###)
    if re.match(r'^#{1,6}\s+', text):
        return True
    
    # 2. numbered listë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ëŠ” ì œëª©ì´ ì•„ë‹˜ (ì‹¤ì œ ì§ˆë¬¸ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
    if re.match(r'^\d+\.\s+\*\*.*\*\*', text):
        return False
        
    # 3. ì§ˆë¬¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ëŠ” ì œëª©ì´ ì•„ë‹˜
    question_keywords = ['ì„¤ëª…í•´ì£¼ì„¸ìš”', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ', 'ë°©ë²•', 'ì°¨ì´ì ', 'ì¥ì ', 'ë‹¨ì ', 
                        'ì˜ˆì‹œ', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ë¹„êµ', 'ì„ íƒ', 'ê³ ë ¤', 'ì ìš©', 'ì‚¬ìš©', '?']
    if any(keyword in text for keyword in question_keywords):
        return False
    
    # 4. ì œëª© í˜•íƒœ íŒ¨í„´ (ì‹¤ì œ ì„¹ì…˜ ì œëª©ë“¤ë§Œ)
    title_patterns = [
        r'^[ê°€-í£\s]*ê¸°ìˆ \s*ë©´ì ‘\s*ì§ˆë¬¸[ê°€-í£\s]*$',  # "ê¸°ìˆ  ë©´ì ‘ ì§ˆë¬¸"ìœ¼ë¡œë§Œ êµ¬ì„±
        r'^[ê°€-í£\s]*ì•„í‚¤í…ì²˜[ê°€-í£\s]*$',           # "ì•„í‚¤í…ì²˜"ë§Œ í¬í•¨í•˜ëŠ” ë‹¨ìˆœ ì œëª©
        r'^[ê°€-í£\s]*ê´€ë ¨\s*ì§ˆë¬¸[ê°€-í£\s]*$',        # "ê´€ë ¨ ì§ˆë¬¸"ìœ¼ë¡œë§Œ êµ¬ì„±
        r'^[ê°€-í£\s]*ë©´ì ‘\s*ë¬¸ì œ[ê°€-í£\s]*$',        # "ë©´ì ‘ ë¬¸ì œ"ë¡œë§Œ êµ¬ì„±
    ]
    
    for pattern in title_patterns:
        if re.match(pattern, text, re.IGNORECASE):
            return True
    
    # 5. ì§ˆë¬¸ì´ ì•„ë‹Œ ì§§ì€ ë¬¸ì¥ (ë¬¼ìŒí‘œê°€ ì—†ê³  ë„ˆë¬´ ì§§ì€ ê²½ìš°)
    if (len(text) < 15 and '?' not in text and 
        not any(keyword in text for keyword in ['ì„¤ëª…', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ'])):
        return True
    
    # 6. ë§ˆí¬ë‹¤ìš´ êµ¬ë¶„ì
    if text in ['---', '***', '===']:
        return True
    
    return False


def is_valid_question(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ ìœ íš¨í•œ ì§ˆë¬¸ì¸ì§€ í™•ì¸
    """
    text = text.strip()
    
    # 1. ìµœì†Œ ê¸¸ì´ í™•ì¸
    if len(text) < 10:
        return False
    
    # 2. ì§ˆë¬¸ í‚¤ì›Œë“œ í™•ì¸
    question_indicators = [
        '?', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ', 'ì„¤ëª…', 'ì°¨ì´ì ', 'ì¥ì ', 'ë‹¨ì ', 
        'ë°©ë²•', 'ì „ëµ', 'êµ¬í˜„', 'ì‚¬ìš©', 'ì ìš©', 'ê³ ë ¤', 'ì²˜ë¦¬', 'í•´ê²°'
    ]
    
    has_question_indicator = any(indicator in text for indicator in question_indicators)
    
    # 3. ì œëª©/í—¤ë”ê°€ ì•„ë‹Œì§€ í™•ì¸
    is_not_header = not is_header_or_title(text)
    
    return has_question_indicator and is_not_header


def parse_compound_question(question: QuestionResponse) -> List[QuestionResponse]:
    """
    ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì„ ì •ë¦¬í•˜ì—¬ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
    
    Args:
        question: ì›ë³¸ ì§ˆë¬¸ ê°ì²´
        
    Returns:
        List[QuestionResponse]: ì •ë¦¬ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    question_text = question.question
    
    # 1. ë§ˆí¬ë‹¤ìš´ ì œëª©ê³¼ ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
    question_text = re.sub(r'^#{1,6}\s+.*$', '', question_text, flags=re.MULTILINE)  # ë§ˆí¬ë‹¤ìš´ ì œëª© ì œê±°
    question_text = re.sub(r'^---+\s*$', '', question_text, flags=re.MULTILINE)     # êµ¬ë¶„ì ì œê±°
    question_text = re.sub(r'\n\s*\n', '\n\n', question_text)                      # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ ì •ë¦¬
    
    # 2. ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
    lines = question_text.split('\n')
    processed_lines = []
    
    for line in lines:
        line_stripped = line.strip()
        if not line_stripped:
            continue
            
        # ë§ˆí¬ë‹¤ìš´ ì œëª© ìŠ¤í‚µ
        if re.match(r'^#{1,6}\s+', line_stripped):
            continue
            
        # numbered list í•­ëª©ì˜ ë²ˆí˜¸ ì œê±° ë° ì •ë¦¬
        clean_line = line_stripped
        
        # ë‹¤ì–‘í•œ numbered list íŒ¨í„´ ì²˜ë¦¬
        patterns_to_remove = [
            r'^\d+\.\s+',      # "1. "
            r'^\d+\)\s+',      # "1) "
            r'^\s*\d+\.\s+',  # "  1. "
        ]
        
        for pattern in patterns_to_remove:
            if re.match(pattern, clean_line):
                clean_line = re.sub(pattern, '', clean_line).strip()
                break
        
        # ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
        if clean_line:
            processed_lines.append(clean_line)
    
    # 3. ì²˜ë¦¬ëœ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì§ˆë¬¸ìœ¼ë¡œ ê²°í•©
    cleaned_question = ' '.join(processed_lines).strip()
    
    # 4. ì •ë¦¬ëœ ì§ˆë¬¸ì´ ìœ íš¨í•œì§€ í™•ì¸
    if (len(cleaned_question) > 20 and 
        any(keyword in cleaned_question for keyword in ['ì„¤ëª…í•´ì£¼ì„¸ìš”', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ', 'ë°©ë²•', 'ì°¨ì´ì ', '?', 'ì˜ˆì‹œ', 'êµ¬ì²´ì '])):
        
        # ì •ë¦¬ëœ ì§ˆë¬¸ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        question.question = cleaned_question
        return [question]
    
    # 5. ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return [question]


def parse_questions_list(questions: List[QuestionResponse]) -> List[QuestionResponse]:
    """
    ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ compound questionë“¤ì„ ë¶„ë¦¬
    
    Args:
        questions: ì›ë³¸ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[QuestionResponse]: íŒŒì‹±ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    parsed_questions = []
    
    for question in questions:
        # ê° ì§ˆë¬¸ì„ íŒŒì‹±í•˜ì—¬ ê²°ê³¼ ì¶”ê°€
        parsed_list = parse_compound_question(question)
        parsed_questions.extend(parsed_list)
    
    return parsed_questions


class QuestionGenerationResult(BaseModel):
    """ì§ˆë¬¸ ìƒì„± ê²°ê³¼"""
    success: bool
    questions: List[QuestionResponse]
    analysis_id: Optional[str] = None
    error: Optional[str] = None


@router.post("/generate", response_model=QuestionGenerationResult)
async def generate_questions(
    request: QuestionGenerationRequest,
    github_token: Optional[str] = Header(None, alias="x-github-token"),
    google_api_key: Optional[str] = Header(None, alias="x-google-api-key")
):
    """GitHub ì €ì¥ì†Œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±"""
    
    try:
        # ë¶„ì„ ê²°ê³¼ì—ì„œ analysis_id ì¶”ì¶œ
        analysis_id = None
        if request.analysis_result and "analysis_id" in request.analysis_result:
            analysis_id = request.analysis_result["analysis_id"]

        # ğŸ”§ í•µì‹¬ ìˆ˜ì •: ì¤‘ë³µ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•œ Redis ë½ ì‚¬ìš©
        if analysis_id and not request.force_regenerate:
            from app.core.database import get_redis
            import asyncio
            
            lock_key = f"question_generation_lock:{analysis_id}"
            lock_timeout = 300  # 5ë¶„ (ì§ˆë¬¸ ìƒì„±ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
            
            try:
                redis = await get_redis()
                
                # ğŸ”¥ Redis ë½ íšë“ ì‹œë„ (ì´ë¯¸ ìƒì„± ì¤‘ì¸ ìš”ì²­ì´ ìˆìœ¼ë©´ ëŒ€ê¸° ë˜ëŠ” ê±°ë¶€)
                lock_acquired = await redis.set(lock_key, "generating", ex=lock_timeout, nx=True)
                
                if not lock_acquired:
                    # ë½ì„ íšë“í•˜ì§€ ëª»í•œ ê²½ìš°: ë‹¤ë¥¸ ìš”ì²­ì´ ì´ë¯¸ ì§„í–‰ ì¤‘
                    print(f"[LOCK_BLOCKED] ì§ˆë¬¸ ìƒì„±ì´ ì´ë¯¸ ì§„í–‰ ì¤‘: analysis_id={analysis_id}")
                    
                    # ì ì‹œ ëŒ€ê¸° í›„ ìºì‹œì—ì„œ ê²°ê³¼ í™•ì¸
                    for attempt in range(10):  # ìµœëŒ€ 10íšŒ ì‹œë„ (ì•½ 50ì´ˆ)
                        await asyncio.sleep(5)
                        
                        # ì •ê·œí™”ëœ ìºì‹œ í‚¤ë¡œ í™•ì¸
                        normalized_cache_key = analysis_id.replace('-', '')
                        if normalized_cache_key in question_cache:
                            cache_data = question_cache[normalized_cache_key]
                            print(f"[LOCK_WAIT_SUCCESS] ëŒ€ê¸° ì¤‘ ì§ˆë¬¸ ìƒì„± ì™„ë£Œë¨: {len(cache_data.parsed_questions)}ê°œ ì§ˆë¬¸")
                            return QuestionGenerationResult(
                                success=True,
                                questions=cache_data.parsed_questions,
                                analysis_id=analysis_id
                            )
                        
                        # í•˜ì´í”ˆ í¬í•¨ í‚¤ë¡œë„ í™•ì¸
                        if analysis_id in question_cache:
                            cache_data = question_cache[analysis_id]
                            print(f"[LOCK_WAIT_SUCCESS] ëŒ€ê¸° ì¤‘ ì§ˆë¬¸ ìƒì„± ì™„ë£Œë¨ (í•˜ì´í”ˆ í‚¤): {len(cache_data.parsed_questions)}ê°œ ì§ˆë¬¸")
                            return QuestionGenerationResult(
                                success=True,
                                questions=cache_data.parsed_questions,
                                analysis_id=analysis_id
                            )
                    
                    # ëŒ€ê¸° ì‹œê°„ì´ ì´ˆê³¼ëœ ê²½ìš°
                    raise HTTPException(
                        status_code=409,
                        detail={
                            "error": "GENERATION_IN_PROGRESS",
                            "message": "ì§ˆë¬¸ ìƒì„±ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                            "analysis_id": analysis_id,
                            "suggestion": "ì§ˆë¬¸ ëª©ë¡ ì¡°íšŒë¥¼ í†µí•´ ìƒì„± ì™„ë£Œ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                        }
                    )
                
                # ë½ íšë“ ì„±ê³µ - ì§ˆë¬¸ ìƒì„± ì§„í–‰
                print(f"[LOCK_ACQUIRED] ì§ˆë¬¸ ìƒì„± ë½ íšë“ ì„±ê³µ: analysis_id={analysis_id}")
                
            except Exception as e:
                print(f"[LOCK_ERROR] Redis ë½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                # Redis ì—°ê²° ì‹¤íŒ¨ ì‹œì—ë„ ì§ˆë¬¸ ìƒì„±ì€ ê³„ì† ì§„í–‰
        
        # ì´ë¯¸ ìƒì„±ëœ ì§ˆë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸ (ê°•ì œ ì¬ìƒì„±ì´ ì•„ë‹Œ ê²½ìš°)
        if analysis_id and not request.force_regenerate:
            # ì •ê·œí™”ëœ í‚¤ë¡œ ë¨¼ì € í™•ì¸
            normalized_cache_key = analysis_id.replace('-', '')
            if normalized_cache_key in question_cache:
                cache_data = question_cache[normalized_cache_key]
                print(f"[CACHE_HIT] ê¸°ì¡´ ì§ˆë¬¸ ë°˜í™˜ (ì •ê·œí™” í‚¤): {len(cache_data.parsed_questions)}ê°œ")
                return QuestionGenerationResult(
                    success=True,
                    questions=cache_data.parsed_questions,
                    analysis_id=analysis_id
                )
            
            # í•˜ì´í”ˆ í¬í•¨ í‚¤ë¡œë„ í™•ì¸
            if analysis_id in question_cache:
                cache_data = question_cache[analysis_id]
                print(f"[CACHE_HIT] ê¸°ì¡´ ì§ˆë¬¸ ë°˜í™˜ (í•˜ì´í”ˆ í‚¤): {len(cache_data.parsed_questions)}ê°œ")
                return QuestionGenerationResult(
                    success=True,
                    questions=cache_data.parsed_questions,
                    analysis_id=analysis_id
                )
        
        # í—¤ë”ì—ì„œ API í‚¤ ì¶”ì¶œ
        api_keys = extract_api_keys_from_headers(github_token, google_api_key)
        
        # ì§ˆë¬¸ ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = QuestionGenerator()
        
        # ì§ˆë¬¸ ìƒì„± ì‹¤í–‰ - QuestionGenerator ë‚´ë¶€ ê¸°ë³¸ê°’ ì‚¬ìš© (3ê°€ì§€ íƒ€ì… ê· ë“± ë¶„ë°°)
        result = await generator.generate_questions(
            repo_url=request.repo_url,
            difficulty_level=request.difficulty,
            question_count=request.question_count,
            question_types=None,  # ê¸°ë³¸ê°’ ["tech_stack", "architecture", "code_analysis"] ì‚¬ìš©
            analysis_data=request.analysis_result,
            api_keys=api_keys  # API í‚¤ ì „ë‹¬
        )
        
        if not result["success"]:
            raise HTTPException(status_code=500, detail=result.get("error", "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨"))
        
        # ì‘ë‹µ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
        questions = []
        for q in result["questions"]:
            questions.append(QuestionResponse(
                id=q.get("id", ""),
                type=q.get("type", "technical"),
                question=q.get("question", ""),
                difficulty=q.get("difficulty", request.difficulty),
                context=q.get("context"),
                time_estimate=q.get("time_estimate", "5ë¶„"),
                code_snippet=q.get("code_snippet"),
                expected_answer_points=q.get("expected_answer_points"),
                technology=q.get("technology"),
                pattern=q.get("pattern")
            ))
        
        # ì§ˆë¬¸ íŒŒì‹± ì²˜ë¦¬ (compound question ë¶„ë¦¬)
        parsed_questions = parse_questions_list(questions)
        
        # ì§ˆë¬¸ ê·¸ë£¹ ê´€ê³„ ìƒì„±
        question_groups = create_question_groups(parsed_questions)
        
        # ìºì‹œì— ì €ì¥ (êµ¬ì¡°í™”ëœ ë°ì´í„°) - UUID ì •ê·œí™”í•˜ì—¬ ì €ì¥
        if analysis_id:
            from datetime import datetime
            # UUID ì •ê·œí™”: í•˜ì´í”ˆ ì œê±°í•˜ì—¬ ì¼ê´€ì„± ìˆê²Œ ì €ì¥
            normalized_cache_key = analysis_id.replace('-', '')
            cache_data = QuestionCacheData(
                original_questions=questions,
                parsed_questions=parsed_questions,
                question_groups=question_groups,
                created_at=datetime.now().isoformat()
            )
            question_cache[normalized_cache_key] = cache_data
            print(f"[CACHE] ì§ˆë¬¸ì„ ìºì‹œì— ì €ì¥: ì›ë³¸í‚¤={analysis_id}, ì •ê·œí™”í‚¤={normalized_cache_key}, ì§ˆë¬¸ìˆ˜={len(parsed_questions)}")
            
            # í•˜ì´í”ˆ ìˆëŠ” í‚¤ë¡œë„ ì €ì¥ (í˜¸í™˜ì„± ë³´ì¥)
            question_cache[analysis_id] = cache_data
            
            # DBì—ë„ ì €ì¥í•˜ì—¬ ì˜êµ¬ ë³´ì¡´
            await _save_questions_to_db(analysis_id, parsed_questions)
        
        # ğŸ”§ Redis ë½ í•´ì œ (ì§ˆë¬¸ ìƒì„± ì™„ë£Œ)
        if analysis_id:
            try:
                from app.core.database import get_redis
                redis = await get_redis()
                lock_key = f"question_generation_lock:{analysis_id}"
                await redis.delete(lock_key)
                print(f"[LOCK_RELEASED] ì§ˆë¬¸ ìƒì„± ë½ í•´ì œ ì™„ë£Œ: analysis_id={analysis_id}")
            except Exception as lock_error:
                print(f"[LOCK_ERROR] Redis ë½ í•´ì œ ì‹¤íŒ¨ (ì§ˆë¬¸ ìƒì„±ì€ ì„±ê³µ): {str(lock_error)}")
        
        return QuestionGenerationResult(
            success=True,
            questions=parsed_questions,
            analysis_id=analysis_id
        )
        
    except Exception as e:
        # ğŸ”§ ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ Redis ë½ í•´ì œ
        if analysis_id:
            try:
                from app.core.database import get_redis
                redis = await get_redis()
                lock_key = f"question_generation_lock:{analysis_id}"
                await redis.delete(lock_key)
                print(f"[LOCK_RELEASED] ì˜ˆì™¸ ë°œìƒìœ¼ë¡œ ì¸í•œ ë½ í•´ì œ: analysis_id={analysis_id}")
            except Exception as lock_error:
                print(f"[LOCK_ERROR] ì˜ˆì™¸ ìƒí™©ì—ì„œ ë½ í•´ì œ ì‹¤íŒ¨: {str(lock_error)}")
                
        return QuestionGenerationResult(
            success=False,
            questions=[],
            error=str(e)
        )


@router.get("/{analysis_id}")
async def get_questions(analysis_id: str):
    """ë¶„ì„ IDë¡œ ì§ˆë¬¸ ì¡°íšŒ"""
    try:
        # UUID ì •ê·œí™”: í•˜ì´í”ˆ ì œê±°í•˜ì—¬ ìºì‹œ í‚¤ì™€ ë§¤ì¹­
        normalized_analysis_id = analysis_id.replace('-', '')
        
        if normalized_analysis_id not in question_cache:
            raise HTTPException(status_code=404, detail="ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        cache_data = question_cache[normalized_analysis_id]
        return {
            "success": True,
            "questions": cache_data.parsed_questions,
            "question_groups": cache_data.question_groups,
            "created_at": cache_data.created_at
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{analysis_id}/groups")
async def get_question_groups(analysis_id: str):
    """ì§ˆë¬¸ ê·¸ë£¹ ì •ë³´ ì¡°íšŒ"""
    try:
        # UUID ì •ê·œí™”: í•˜ì´í”ˆ ì œê±°í•˜ì—¬ ìºì‹œ í‚¤ì™€ ë§¤ì¹­
        normalized_analysis_id = analysis_id.replace('-', '')
        
        if normalized_analysis_id not in question_cache:
            raise HTTPException(status_code=404, detail="ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        cache_data = question_cache[normalized_analysis_id]
        return {
            "success": True,
            "question_groups": cache_data.question_groups,
            "total_questions": len(cache_data.parsed_questions),
            "total_groups": len(cache_data.question_groups)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/cache/clear")
async def clear_question_cache():
    """ì§ˆë¬¸ ìºì‹œ ì´ˆê¸°í™”"""
    try:
        global question_cache
        cache_count = len(question_cache)
        question_cache.clear()
        
        return {
            "success": True,
            "message": f"ì§ˆë¬¸ ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ({cache_count}ê°œ í•­ëª© ì œê±°)",
            "cleared_count": cache_count
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/cache/status")
async def get_cache_status():
    """ì§ˆë¬¸ ìºì‹œ ìƒíƒœ ì¡°íšŒ"""
    try:
        cache_info = {}
        for analysis_id, cache_data in question_cache.items():
            cache_info[analysis_id] = {
                "original_questions_count": len(cache_data.original_questions),
                "parsed_questions_count": len(cache_data.parsed_questions),
                "groups_count": len(cache_data.question_groups),
                "created_at": cache_data.created_at
            }
        
        return {
            "success": True,
            "total_cached_analyses": len(question_cache),
            "cache_details": cache_info
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/analysis/{analysis_id}")
async def get_questions_by_analysis(analysis_id: str):
    """ë¶„ì„ IDë¡œ ìƒì„±ëœ ì§ˆë¬¸ ì¡°íšŒ - ë©”ëª¨ë¦¬ ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ DB ì¡°íšŒ"""
    try:
        # 1. ë¨¼ì € ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ì¡°íšŒ (UUID ì •ê·œí™”)
        normalized_analysis_id = analysis_id.replace('-', '')
        if normalized_analysis_id in question_cache:
            print(f"[QUESTIONS] Found questions in memory cache for {analysis_id} (normalized: {normalized_analysis_id})")
            cache_data = question_cache[normalized_analysis_id]
            
            # ìºì‹œ ë°ì´í„° êµ¬ì¡° í™•ì¸
            questions = []
            if hasattr(cache_data, 'parsed_questions'):
                questions = cache_data.parsed_questions
            elif hasattr(cache_data, 'questions'):
                questions = cache_data.questions
            elif isinstance(cache_data, list):
                questions = cache_data
            
            return QuestionGenerationResult(
                success=True,
                questions=questions,
                analysis_id=analysis_id
            )
        
        # 2. ë©”ëª¨ë¦¬ ìºì‹œì— ì—†ìœ¼ë©´ DBì—ì„œ ì¡°íšŒ
        print(f"[QUESTIONS] Memory cache miss, checking database for {analysis_id}")
        db_questions = await _load_questions_from_db(analysis_id)
        
        if db_questions:
            print(f"[QUESTIONS] Found {len(db_questions)} questions in database, restoring to cache")
            
            # DBì—ì„œ ê°€ì ¸ì˜¨ ì§ˆë¬¸ë“¤ì„ ë©”ëª¨ë¦¬ ìºì‹œì— ë³µì›
            await _restore_questions_to_cache(analysis_id, db_questions)
            
            return QuestionGenerationResult(
                success=True,
                questions=db_questions,
                analysis_id=analysis_id
            )
        
        # 3. ë©”ëª¨ë¦¬ ìºì‹œì™€ DB ëª¨ë‘ì— ì—†ìŒ
        print(f"[QUESTIONS] No questions found for {analysis_id} in cache or database")
        return QuestionGenerationResult(
            success=False,
            questions=[],
            analysis_id=analysis_id,
            error="í•´ë‹¹ ë¶„ì„ IDì— ëŒ€í•œ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."
        )
        
    except Exception as e:
        print(f"Error in get_questions_by_analysis: {e}")
        return QuestionGenerationResult(
            success=False,
            questions=[],
            analysis_id=analysis_id,
            error=f"ì§ˆë¬¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/types")
async def get_question_types():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì§ˆë¬¸ íƒ€ì… ëª©ë¡ ì¡°íšŒ"""
    return {
        "question_types": [
            "code_analysis",
            "tech_stack", 
            "architecture",
            "design_patterns",
            "problem_solving",
            "best_practices"
        ],
        "difficulties": ["easy", "medium", "hard"]
    }


@router.get("/debug/cache")
async def debug_question_cache():
    """ì§ˆë¬¸ ìºì‹œ ìƒíƒœ í™•ì¸ (ë””ë²„ê¹…ìš©)"""
    return {
        "cache_size": len(question_cache),
        "cached_analysis_ids": list(question_cache.keys()),
        "cache_details": [
            {
                "analysis_id": analysis_id,
                "original_question_count": len(cache_data.original_questions) if hasattr(cache_data, 'original_questions') else 0,
                "parsed_question_count": len(cache_data.parsed_questions) if hasattr(cache_data, 'parsed_questions') else 0,
                "question_types": list(set(q.type for q in cache_data.parsed_questions)) if hasattr(cache_data, 'parsed_questions') else []
            }
            for analysis_id, cache_data in question_cache.items()
        ]
    }


@router.get("/debug/original/{analysis_id}")
async def debug_original_questions(analysis_id: str):
    """ì›ë³¸ ì§ˆë¬¸ í™•ì¸ (ë””ë²„ê¹…ìš©)"""
    try:
        # UUID ì •ê·œí™”: í•˜ì´í”ˆ ì œê±°í•˜ì—¬ ìºì‹œ í‚¤ì™€ ë§¤ì¹­
        normalized_analysis_id = analysis_id.replace('-', '')
        
        if normalized_analysis_id not in question_cache:
            raise HTTPException(status_code=404, detail="ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        cache_data = question_cache[normalized_analysis_id]
        return {
            "success": True,
            "original_questions": [
                {
                    "id": q.id,
                    "type": q.type, 
                    "question": q.question,
                    "is_compound": q.is_compound_question,
                    "total_sub_questions": q.total_sub_questions
                }
                for q in cache_data.original_questions
            ],
            "parsed_questions_count": len(cache_data.parsed_questions),
            "groups_count": len(cache_data.question_groups)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/debug/add-test-questions/{analysis_id}")
async def add_test_questions(analysis_id: str):
    """í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ ì¶”ê°€ (ë””ë²„ê¹…ìš©)"""
    try:
        from datetime import datetime
        
        # í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ ìƒì„±
        test_questions = [
            QuestionResponse(
                id=str(uuid.uuid4()),
                type="technical",
                question="Linux ì»¤ë„ì˜ ì£¼ìš” ì„œë¸Œì‹œìŠ¤í…œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                difficulty="medium",
                context="Linux ì»¤ë„ ì•„í‚¤í…ì²˜",
                time_estimate="5ë¶„",
                technology="C"
            ),
            QuestionResponse(
                id=str(uuid.uuid4()),
                type="architecture",
                question="1. ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„œë¸Œì‹œìŠ¤í…œì˜ ì—­í• ì€?\n2. í”„ë¡œì„¸ìŠ¤ ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ë™ì‘ ì›ë¦¬ëŠ”?\n3. íŒŒì¼ ì‹œìŠ¤í…œì˜ VFS ë ˆì´ì–´ ëª©ì ì€?",
                difficulty="medium",
                context="Linux ì»¤ë„ ì•„í‚¤í…ì²˜",
                time_estimate="10ë¶„",
                technology="C"
            ),
            QuestionResponse(
                id=str(uuid.uuid4()),
                type="code_analysis",
                question="ë””ë°”ì´ìŠ¤ ë“œë¼ì´ë²„ë¥¼ ì‘ì„±í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ì£¼ìš” ìš”ì†Œë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                difficulty="medium",
                context="Linux ì»¤ë„ ê°œë°œ",
                time_estimate="7ë¶„",
                technology="C"
            )
        ]
        
        # ì§ˆë¬¸ íŒŒì‹± ì²˜ë¦¬
        parsed_questions = parse_questions_list(test_questions)
        
        # ì§ˆë¬¸ ê·¸ë£¹ ê´€ê³„ ìƒì„±
        question_groups = create_question_groups(parsed_questions)
        
        # ìºì‹œì— ì €ì¥ (UUID ì •ê·œí™”)
        normalized_cache_key = analysis_id.replace('-', '')
        cache_data = QuestionCacheData(
            original_questions=test_questions,
            parsed_questions=parsed_questions,
            question_groups=question_groups,
            created_at=datetime.now().isoformat()
        )
        question_cache[normalized_cache_key] = cache_data
        # í˜¸í™˜ì„±ì„ ìœ„í•´ ì›ë³¸ í‚¤ë¡œë„ ì €ì¥
        question_cache[analysis_id] = cache_data
        
        return {
            "success": True,
            "message": f"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. (ì›ë³¸: {len(test_questions)}, íŒŒì‹±: {len(parsed_questions)})",
            "analysis_id": analysis_id,
            "questions": parsed_questions
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/cache")
async def debug_question_cache():
    """ì§ˆë¬¸ ìºì‹œ ìƒíƒœ í™•ì¸ (ë””ë²„ê¹…ìš©)"""
    return {
        "cache_size": len(question_cache),
        "cached_analysis_ids": list(question_cache.keys()),
        "cache_details": [
            {
                "analysis_id": analysis_id,
                "question_count": len(cache_data.parsed_questions),
                "created_at": cache_data.created_at
            }
            for analysis_id, cache_data in question_cache.items()
        ]
    }


@router.delete("/debug/cache")
async def clear_question_cache():
    """ì§ˆë¬¸ ìºì‹œ ì´ˆê¸°í™” (ë””ë²„ê¹…ìš©)"""
    cache_size_before = len(question_cache)
    question_cache.clear()
    
    return {
        "message": "ì§ˆë¬¸ ìºì‹œê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤",
        "cleared_items": cache_size_before,
        "current_cache_size": len(question_cache)
    }


async def _load_questions_from_db(analysis_id: str) -> List[QuestionResponse]:
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì§ˆë¬¸ ì¡°íšŒ"""
    try:
        with engine.connect() as conn:
            # InterviewQuestion í…Œì´ë¸”ì—ì„œ ì§ˆë¬¸ ì¡°íšŒ
            result = conn.execute(text(
                """
                SELECT id, category, difficulty, question_text, expected_points, 
                       related_files, context, created_at
                FROM interview_questions 
                WHERE analysis_id = :analysis_id
                ORDER BY created_at ASC
                """
            ), {"analysis_id": analysis_id})
            
            questions = []
            for row in result:
                # expected_points JSON íŒŒì‹±
                expected_points = None
                if row[4]:  # expected_points í•„ë“œ
                    try:
                        expected_points = json.loads(row[4]) if isinstance(row[4], str) else row[4]
                    except json.JSONDecodeError:
                        expected_points = None
                
                # ë°ì´í„°ë² ì´ìŠ¤ rowë¥¼ QuestionResponse ê°ì²´ë¡œ ë³€í™˜
                question = QuestionResponse(
                    id=str(row[0]),
                    type=row[1],  # category -> type
                    question=row[3],  # question_text -> question
                    difficulty=row[2],
                    context=None,  # contextëŠ” JSONì´ë¯€ë¡œ ê°„ë‹¨íˆ Noneìœ¼ë¡œ ì²˜ë¦¬
                    time_estimate="5ë¶„",  # ê¸°ë³¸ê°’
                    code_snippet=None,
                    expected_answer_points=expected_points,
                    technology=None,
                    pattern=None
                )
                questions.append(question)
            
            print(f"[DB] Loaded {len(questions)} questions from database for analysis {analysis_id}")
            return questions
            
    except Exception as e:
        print(f"[DB] Error loading questions from database: {e}")
        return []


async def _restore_questions_to_cache(analysis_id: str, questions: List[QuestionResponse]):
    """DBì—ì„œ ê°€ì ¸ì˜¨ ì§ˆë¬¸ë“¤ì„ ë©”ëª¨ë¦¬ ìºì‹œì— ë³µì›"""
    try:
        from datetime import datetime
        
        # ì§ˆë¬¸ ê·¸ë£¹ ê´€ê³„ ìƒì„±
        question_groups = create_question_groups(questions)
        
        # ìºì‹œì— ì €ì¥í•  ë°ì´í„° êµ¬ì¡° ìƒì„±
        cache_data = QuestionCacheData(
            original_questions=questions,  # DBì—ì„œ ê°€ì ¸ì˜¨ ì§ˆë¬¸ë“¤ì„ ì›ë³¸ìœ¼ë¡œ ì²˜ë¦¬
            parsed_questions=questions,    # ì´ë¯¸ íŒŒì‹±ëœ ìƒíƒœë¡œ ê°„ì£¼
            question_groups=question_groups,
            created_at=datetime.now().isoformat()
        )
        
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥ (UUID ì •ê·œí™”í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€)
        normalized_cache_key = analysis_id.replace('-', '')
        question_cache[normalized_cache_key] = cache_data
        # í˜¸í™˜ì„±ì„ ìœ„í•´ ì›ë³¸ í‚¤ë¡œë„ ì €ì¥
        question_cache[analysis_id] = cache_data
        
        print(f"[CACHE] Restored {len(questions)} questions to memory cache for analysis {analysis_id} (normalized: {normalized_cache_key})")
        
    except Exception as e:
        print(f"[CACHE] Error restoring questions to cache: {e}")


async def _save_questions_to_db(analysis_id: str, questions: List[QuestionResponse]):
    """ìƒì„±ëœ ì§ˆë¬¸ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ - UPSERT ë°©ì‹ìœ¼ë¡œ ê°œì„ """
    try:
        from app.core.database import database_url
        from datetime import datetime
        current_time = datetime.now()
        
        with engine.connect() as conn:
            # ğŸ”§ í•µì‹¬ ê°œì„ : DELETE-INSERT ëŒ€ì‹  UPSERT ì‚¬ìš©
            # ë°ì´í„°ë² ì´ìŠ¤ ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ë¥¸ UPSERT êµ¬ë¬¸ ì‚¬ìš©
            is_sqlite = "sqlite" in database_url.lower()
            
            print(f"[DB_UPSERT] UPSERT ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸ ì €ì¥ ì‹œì‘: {len(questions)}ê°œ ì§ˆë¬¸, DBíƒ€ì…={'SQLite' if is_sqlite else 'PostgreSQL'}")
            
            # ğŸ”¥ ë‹¨ê³„ë³„ UPSERT: ê¸°ì¡´ ì§ˆë¬¸ ID ì¡°íšŒ í›„ ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
            for question in questions:
                if is_sqlite:
                    # SQLite: INSERT OR REPLACE ì‚¬ìš© (updated_at ì»¬ëŸ¼ ì—†ìŒ)
                    conn.execute(text(
                        """
                        INSERT OR REPLACE INTO interview_questions 
                        (id, analysis_id, category, difficulty, question_text, expected_points, created_at)
                        VALUES (:id, :analysis_id, :category, :difficulty, :question_text, :expected_points, 
                                COALESCE((SELECT created_at FROM interview_questions WHERE id = :id), :created_at))
                        """
                    ), {
                        "id": question.id,
                        "analysis_id": analysis_id,
                        "category": question.type,
                        "difficulty": question.difficulty,
                        "question_text": question.question,
                        "expected_points": json.dumps(question.expected_answer_points) if question.expected_answer_points else None,
                        "created_at": current_time
                    })
                else:
                    # PostgreSQL: INSERT ... ON CONFLICT DO UPDATE ì‚¬ìš© (updated_at ì»¬ëŸ¼ ì—†ìŒ)
                    conn.execute(text(
                        """
                        INSERT INTO interview_questions 
                        (id, analysis_id, category, difficulty, question_text, expected_points, created_at)
                        VALUES (:id, :analysis_id, :category, :difficulty, :question_text, :expected_points, :created_at)
                        ON CONFLICT (id) DO UPDATE SET
                            category = EXCLUDED.category,
                            difficulty = EXCLUDED.difficulty,
                            question_text = EXCLUDED.question_text,
                            expected_points = EXCLUDED.expected_points
                        """
                    ), {
                        "id": question.id,
                        "analysis_id": analysis_id,
                        "category": question.type,
                        "difficulty": question.difficulty,
                        "question_text": question.question,
                        "expected_points": json.dumps(question.expected_answer_points) if question.expected_answer_points else None,
                        "created_at": current_time
                    })
            
            # ğŸ”§ ì¶”ê°€ ê°œì„ : í˜„ì¬ ì§ˆë¬¸ ì„¸íŠ¸ì— ì—†ëŠ” ê¸°ì¡´ ì§ˆë¬¸ë“¤ì€ ë¹„í™œì„±í™” (ì‚­ì œí•˜ì§€ ì•ŠìŒ)
            current_question_ids = [q.id for q in questions]
            if current_question_ids:
                question_ids_placeholder = ','.join([f"'{qid}'" for qid in current_question_ids])
                
                # í˜„ì¬ ì§ˆë¬¸ ì„¸íŠ¸ì— ì—†ëŠ” ê¸°ì¡´ ì§ˆë¬¸ë“¤ ë¹„í™œì„±í™”
                result = conn.execute(text(
                    f"""
                    UPDATE interview_questions 
                    SET is_active = FALSE, updated_at = :updated_at
                    WHERE analysis_id = :analysis_id 
                    AND id NOT IN ({question_ids_placeholder})
                    AND is_active = TRUE
                    """
                ), {
                    "analysis_id": analysis_id,
                    "updated_at": current_time
                })
                
                deactivated_count = result.rowcount if hasattr(result, 'rowcount') else 0
                if deactivated_count > 0:
                    print(f"[DB_UPSERT] ê¸°ì¡´ ì§ˆë¬¸ {deactivated_count}ê°œ ë¹„í™œì„±í™” (ì‚­ì œí•˜ì§€ ì•ŠìŒ)")
            
            # ë³€ê²½ì‚¬í•­ ì»¤ë°‹
            conn.commit()
            
            print(f"[DB_UPSERT] UPSERT ì™„ë£Œ: {len(questions)}ê°œ ì§ˆë¬¸ ì €ì¥/ì—…ë°ì´íŠ¸, analysis_id={analysis_id}")
            print(f"[DB_UPSERT] âœ… ì¥ì : ì§ˆë¬¸ ì‚­ì œ ì—†ì´ ì›ìì  ì—…ë°ì´íŠ¸ë¡œ ìºì‹œ-DB ì¼ê´€ì„± ë³´ì¥")
            
    except Exception as e:
        print(f"[DB_UPSERT_ERROR] UPSERT ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        print(f"[DB_UPSERT_ERROR] í´ë°±: ê¸°ì¡´ DELETE-INSERT ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
        
        # ğŸ”§ í´ë°±: UPSERT ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
        try:
            with engine.connect() as conn:
                # ê¸°ì¡´ ì§ˆë¬¸ ì‚­ì œ
                conn.execute(text(
                    "DELETE FROM interview_questions WHERE analysis_id = :analysis_id"
                ), {"analysis_id": analysis_id})
                
                # ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤ ì €ì¥
                from datetime import datetime
                current_time = datetime.now()
                
                for question in questions:
                    conn.execute(text(
                        """
                        INSERT INTO interview_questions 
                        (id, analysis_id, category, difficulty, question_text, expected_points, created_at)
                        VALUES (:id, :analysis_id, :category, :difficulty, :question_text, :expected_points, :created_at)
                        """
                    ), {
                        "id": question.id,
                        "analysis_id": analysis_id,
                        "category": question.type,
                        "difficulty": question.difficulty,
                        "question_text": question.question,
                        "expected_points": json.dumps(question.expected_answer_points) if question.expected_answer_points else None,
                        "created_at": current_time
                    })
                
                conn.commit()
                print(f"[DB_FALLBACK] í´ë°± ì €ì¥ ì„±ê³µ: {len(questions)}ê°œ ì§ˆë¬¸")
                
        except Exception as fallback_error:
            print(f"[DB_FALLBACK_ERROR] í´ë°±ë„ ì‹¤íŒ¨: {str(fallback_error)}")
            # DB ì €ì¥ ì‹¤íŒ¨ëŠ” ì§ˆë¬¸ ìƒì„± ìì²´ë¥¼ ì‹¤íŒ¨ì‹œí‚¤ì§€ ì•ŠìŒ