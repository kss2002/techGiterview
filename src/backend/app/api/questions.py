"""
Question Generation API Router

ì§ˆë¬¸ ìƒì„± ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸
"""

from typing import Dict, List, Any, Optional
import re
import uuid
import json
from html import unescape
from fastapi import APIRouter, HTTPException, Depends, Header
from pydantic import BaseModel
from sqlalchemy import text

from app.agents.question_generator import QuestionGenerator
from app.core.config import settings
from app.core.database import engine

router = APIRouter()

# ì§ˆë¬¸ ìºì‹œ (ë¶„ì„ IDë³„ë¡œ ì§ˆë¬¸ ì €ì¥)
question_cache = {}


# API í‚¤ ì¶”ì¶œ í•¨ìˆ˜ëŠ” app.core.api_utilsë¡œ ì´ë™ë¨





class QuestionGenerationRequest(BaseModel):
    """ì§ˆë¬¸ ìƒì„± ìš”ì²­"""
    repo_url: str
    analysis_result: Optional[Dict[str, Any]] = None
    question_type: str = "technical"
    difficulty: str = "medium"
    question_count: int = 9
    force_regenerate: bool = False  # ê°•ì œ ì¬ìƒì„± ì˜µì…˜


class QuestionResponse(BaseModel):
    """ì§ˆë¬¸ ì‘ë‹µ"""
    id: str
    type: str
    question: str
    difficulty: str
    context: Optional[str] = None
    time_estimate: Optional[str] = None
    code_snippet: Optional[Dict[str, Any]] = None
    expected_answer_points: Optional[List[str]] = None
    technology: Optional[str] = None
    pattern: Optional[str] = None
    # ì„œë¸Œ ì§ˆë¬¸ ê´€ë ¨ í•„ë“œ
    parent_question_id: Optional[str] = None
    sub_question_index: Optional[int] = None
    total_sub_questions: Optional[int] = None
    is_compound_question: bool = False
    # ì •ê·œí™”ëœ ë Œë”ë§ í•„ë“œ (í˜¸í™˜ì„±ì„ ìœ„í•´ optional)
    question_headline: Optional[str] = None
    question_details_markdown: Optional[str] = None
    question_has_details: Optional[bool] = None


class QuestionCacheData(BaseModel):
    """ì§ˆë¬¸ ìºì‹œ ë°ì´í„° êµ¬ì¡°"""
    original_questions: List[QuestionResponse]  # AI ì›ë³¸ ì§ˆë¬¸
    parsed_questions: List[QuestionResponse]   # íŒŒì‹±ëœ ê°œë³„ ì§ˆë¬¸
    question_groups: Dict[str, List[str]]      # ê·¸ë£¹ë³„ ì§ˆë¬¸ ê´€ê³„ (parent_id -> [sub_question_ids])
    created_at: str


def create_question_groups(questions: List[QuestionResponse]) -> Dict[str, List[str]]:
    """ì§ˆë¬¸ ê·¸ë£¹ ê´€ê³„ ìƒì„±"""
    groups = {}
    
    for question in questions:
        if question.parent_question_id:
            parent_id = question.parent_question_id
            if parent_id not in groups:
                groups[parent_id] = []
            groups[parent_id].append(question.id)
    
    return groups


QUESTION_SECTION_LABELS = [
    "ì§ˆë¬¸",
    "ìƒí™©",
    "ìš”êµ¬ì‚¬í•­",
    "í‰ê°€ í¬ì¸íŠ¸",
    "ì¶”ê°€ ì§ˆë¬¸ í¬ì¸íŠ¸"
]


def _normalize_inline_spaces(value: str) -> str:
    return re.sub(r'\s+', ' ', value).strip()


def _strip_outer_question_wrapper_html(text: str) -> str:
    """`<div class=\"question-text\">...</div>` ê°™ì€ ì™¸ê³½ ë˜í¼ ì œê±°"""
    if not text:
        return ""

    current = text.strip()
    wrapper_pattern = re.compile(
        r'^\s*<div[^>]*class=["\'][^"\']*\bquestion-text\b[^"\']*["\'][^>]*>([\s\S]*)</div>\s*$',
        re.IGNORECASE
    )

    while True:
        match = wrapper_pattern.match(current)
        if not match:
            break
        current = match.group(1).strip()

    return current


def _convert_question_html_to_markdown(text: str) -> str:
    """ì§ˆë¬¸ ë Œë”ë§ì— ìì£¼ ë“±ì¥í•˜ëŠ” HTMLì„ markdown/textë¡œ ì •ë¦¬"""
    if not text:
        return ""

    converted = text

    # ì„¹ì…˜ strong/b íƒœê·¸ë¥¼ markdown ì„¹ì…˜ í—¤ë”ë¡œ ì •ê·œí™”
    for label in QUESTION_SECTION_LABELS:
        converted = re.sub(
            rf'(?is)<(?:strong|b)>\s*{re.escape(label)}\s*[:ï¼š]\s*</(?:strong|b)>',
            f'**{label}:** ',
            converted
        )

    # ì¤„ë°”ê¿ˆ/ë¦¬ìŠ¤íŠ¸ ê´€ë ¨ íƒœê·¸ ì •ë¦¬
    converted = re.sub(r'(?i)<br\s*/?>', '\n', converted)
    converted = re.sub(r'(?i)</p\s*>', '\n', converted)
    converted = re.sub(r'(?i)<p[^>]*>', '', converted)
    converted = re.sub(r'(?i)</li\s*>', '\n', converted)
    converted = re.sub(r'(?i)<li[^>]*>', '- ', converted)
    converted = re.sub(r'(?i)</div\s*>', '\n', converted)
    converted = re.sub(r'(?i)<div[^>]*>', '', converted)

    # ë‚˜ë¨¸ì§€ HTML íƒœê·¸ ì œê±°
    converted = re.sub(r'<[^>]+>', '', converted)

    converted = unescape(converted)
    converted = converted.replace('\r\n', '\n').replace('\r', '\n')
    converted = re.sub(r'\n{3,}', '\n\n', converted)

    return converted.strip()


def _dedupe_mirrored_text(text: str) -> str:
    """ë¬¸ìì—´ì´ í†µì§¸ë¡œ ë‘ ë²ˆ ë°˜ë³µëœ ì¼€ì´ìŠ¤ ì œê±°"""
    if not text:
        return ""

    stripped = text.strip()
    if len(stripped) < 20:
        return stripped

    # ì •í™•íˆ ê°™ì€ ë¸”ë¡ì´ ì—°ì† ë°˜ë³µëœ ê²½ìš°(ì¤‘ê°„ ê³µë°± í—ˆìš©)
    repeated_block_match = re.match(r'(?s)^\s*(.{200,}?)\s*\1\s*$', stripped)
    if repeated_block_match:
        return repeated_block_match.group(1).strip()

    # ì •í™•íˆ ë°˜ë°˜ ë°˜ë³µëœ ê²½ìš°
    if len(stripped) % 2 == 0:
        half = len(stripped) // 2
        first = stripped[:half].strip()
        second = stripped[half:].strip()
        if first and _normalize_inline_spaces(first) == _normalize_inline_spaces(second):
            return first

    return stripped


def _canonicalize_section_content(text: str) -> str:
    normalized = re.sub(r'[*_`>#-]', ' ', text or '')
    return _normalize_inline_spaces(normalized).lower()


def _clean_section_line(value: str) -> str:
    line = (value or "").strip()
    if not line:
        return ""

    # ë¦¬ìŠ¤íŠ¸/ê°•ì¡° ë§ˆí¬ë‹¤ìš´ ì”ì¬ ì œê±°
    line = re.sub(r'^\s*[-*+]\s*', '', line)
    line = re.sub(r'^\*\*+\s*', '', line)
    line = re.sub(r'\s*\*\*+$', '', line)
    line = re.sub(r'^\s*[:ï¼š]\s*', '', line)
    line = _normalize_inline_spaces(line)
    return line


def _merge_section_contents(contents: List[str]) -> str:
    lines: List[str] = []
    seen = set()
    for content in contents:
        for raw_line in re.split(r'\n+', content or ''):
            cleaned = _clean_section_line(raw_line)
            if not cleaned:
                continue
            key = _canonicalize_section_content(cleaned)
            if not key or key in seen:
                continue
            seen.add(key)
            lines.append(cleaned)

    if not lines:
        return ""
    if len(lines) == 1:
        return lines[0]
    return "\n".join(f"- {line}" for line in lines)


def _extract_headline(text: str) -> str:
    if not text:
        return ""

    lines = [line.strip() for line in text.split('\n') if line.strip()]
    if not lines:
        return ""

    line = re.sub(r'^[>\-\*\d\.\)\s]+', '', lines[0]).strip()
    if not line:
        return ""

    question_mark_idx = line.find('?')
    if question_mark_idx != -1 and question_mark_idx < 180:
        return line[:question_mark_idx + 1].strip()

    return line


def _remove_headline_prefix(full_text: str, headline: str) -> str:
    if not full_text or not headline:
        return full_text.strip() if full_text else ""

    stripped = full_text.strip()
    if stripped.startswith(headline):
        remainder = stripped[len(headline):].lstrip(" \n\t:-")
        return remainder.strip()

    return stripped


def normalize_question_payload(raw_question: Optional[str]) -> Dict[str, Any]:
    """
    ì§ˆë¬¸ ë¬¸ìì—´ì„ ì •ê·œí™”í•˜ì—¬ headline/detailsë¥¼ ì¶”ì¶œ.
    - DB ë°ì´í„°ëŠ” ìœ ì§€í•˜ê³  read/response ì‹œì ì—ì„œë§Œ ì •ê·œí™”í•œë‹¤.
    """
    raw = (raw_question or "").strip()
    if not raw:
        return {
            "question": "",
            "question_headline": None,
            "question_details_markdown": None,
            "question_has_details": False
        }

    text = _strip_outer_question_wrapper_html(raw)
    text = _convert_question_html_to_markdown(text)
    text = _dedupe_mirrored_text(text)

    # markdown ê°•ì¡° í˜•íƒœì˜ ì„¹ì…˜ í—¤ë” ì •ê·œí™”
    text = re.sub(
        r'\*\*\s*(ì§ˆë¬¸|ìƒí™©|ìš”êµ¬ì‚¬í•­|í‰ê°€ í¬ì¸íŠ¸|ì¶”ê°€ ì§ˆë¬¸ í¬ì¸íŠ¸)\s*[:ï¼š]\s*\*\*',
        r'\1:',
        text,
        flags=re.IGNORECASE
    )
    text = re.sub(
        r'\*\*\s*(ì§ˆë¬¸|ìƒí™©|ìš”êµ¬ì‚¬í•­|í‰ê°€ í¬ì¸íŠ¸|ì¶”ê°€ ì§ˆë¬¸ í¬ì¸íŠ¸)\s*\*\*\s*[:ï¼š]',
        r'\1:',
        text,
        flags=re.IGNORECASE
    )

    # ì„¹ì…˜ í—¤ë”ê°€ í•œ ì¤„ë¡œ ë¶™ì–´ì˜¤ëŠ” ê²½ìš°ë¥¼ ìœ„í•´ ì¤„ë°”ê¿ˆ ì‚½ì…
    text = re.sub(
        r'\s*((?:\*\*)?\s*(ì§ˆë¬¸|ìƒí™©|ìš”êµ¬ì‚¬í•­|í‰ê°€ í¬ì¸íŠ¸|ì¶”ê°€ ì§ˆë¬¸ í¬ì¸íŠ¸)\s*(?:\*\*)?\s*[:ï¼š])',
        r'\n\1',
        text,
        flags=re.IGNORECASE
    )
    text = re.sub(r'\n{3,}', '\n\n', text).strip()

    section_pattern = re.compile(
        r'(?im)^\s*(?:[-*]\s*)?(?:\*\*)?\s*(ì§ˆë¬¸|ìƒí™©|ìš”êµ¬ì‚¬í•­|í‰ê°€ í¬ì¸íŠ¸|ì¶”ê°€ ì§ˆë¬¸ í¬ì¸íŠ¸)\s*(?:\*\*)?\s*[:ï¼š]\s*'
    )
    matches = list(section_pattern.finditer(text))

    ordered_sections: List[tuple[str, str]] = []
    seen_section_content = set()
    for idx, match in enumerate(matches):
        start = match.end()
        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)
        label = match.group(1).strip()
        content = text[start:end].strip()
        if not content:
            continue

        canonical = (label, _canonicalize_section_content(content))
        if canonical in seen_section_content:
            continue
        seen_section_content.add(canonical)
        ordered_sections.append((label, content))

    headline = ""
    details_markdown = ""

    if ordered_sections:
        # ë™ì¼ ì„¹ì…˜ì€ í•˜ë‚˜ë¡œ ë³‘í•©í•´ ì¤‘ë³µ í—¤ë”/ê¹¨ì§„ ëª©ë¡ ì œê±°
        grouped_sections: Dict[str, List[str]] = {}
        ordered_labels: List[str] = []
        for label, content in ordered_sections:
            if label not in grouped_sections:
                grouped_sections[label] = []
                ordered_labels.append(label)
            grouped_sections[label].append(content)

        merged_sections: List[tuple[str, str]] = []
        for label in ordered_labels:
            merged = _merge_section_contents(grouped_sections[label])
            if merged:
                merged_sections.append((label, merged))

        question_section = next((content for label, content in merged_sections if label == "ì§ˆë¬¸"), "")
        fallback_source = question_section or ordered_sections[0][1]
        headline = _extract_headline(fallback_source)

        detail_parts: List[str] = []

        # ì§ˆë¬¸ ì„¹ì…˜ ë³¸ë¬¸ì—ì„œ headline ì´í›„ì˜ ì¶”ê°€ ì„¤ëª… ë³´ì¡´
        question_remainder = _remove_headline_prefix(question_section, headline) if question_section else ""
        if question_remainder:
            detail_parts.append(f"**ì§ˆë¬¸ ìƒì„¸:**\n{question_remainder}")

        for label, content in merged_sections:
            if label == "ì§ˆë¬¸":
                continue
            detail_parts.append(f"**{label}:**\n{content}")

        details_markdown = "\n\n".join(part for part in detail_parts if part.strip()).strip()
    else:
        # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ë¬¸ë‹¨ ì¤‘ë³µ ì œê±° í›„ headline ì¶”ì¶œ
        paragraphs = [p.strip() for p in re.split(r'\n{2,}', text) if p.strip()]
        deduped_paragraphs: List[str] = []
        seen_paragraphs = set()
        for paragraph in paragraphs:
            key = _canonicalize_section_content(paragraph)
            if not key or key in seen_paragraphs:
                continue
            seen_paragraphs.add(key)
            deduped_paragraphs.append(paragraph)

        plain = "\n\n".join(deduped_paragraphs).strip() or text
        headline = _extract_headline(plain)
        remainder = _remove_headline_prefix(plain, headline)
        details_markdown = remainder if _canonicalize_section_content(remainder) != _canonicalize_section_content(headline) else ""

    headline = headline.strip()
    details_markdown = details_markdown.strip()
    has_details = bool(details_markdown)

    normalized_question = headline or text
    if has_details:
        normalized_question = f"{headline}\n\n{details_markdown}" if headline else details_markdown

    return {
        "question": normalized_question.strip(),
        "question_headline": headline or None,
        "question_details_markdown": details_markdown or None,
        "question_has_details": has_details
    }


def normalize_question_response(question: Any) -> QuestionResponse:
    if isinstance(question, QuestionResponse):
        normalized_question = question
    elif isinstance(question, dict):
        normalized_question = QuestionResponse(**question)
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì§ˆë¬¸ ë°ì´í„° íƒ€ì…: {type(question)}")

    normalized = normalize_question_payload(normalized_question.question)
    normalized_question.question = normalized["question"]
    normalized_question.question_headline = normalized["question_headline"]
    normalized_question.question_details_markdown = normalized["question_details_markdown"]
    normalized_question.question_has_details = normalized["question_has_details"]
    return normalized_question


def is_header_or_title(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ ì œëª©ì´ë‚˜ í—¤ë”ì¸ì§€ í™•ì¸
    """
    text = text.strip()
    
    # 1. ë§ˆí¬ë‹¤ìš´ í—¤ë” íŒ¨í„´ (#, ##, ###)
    if re.match(r'^#{1,6}\s+', text):
        return True
    
    # 2. numbered listë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ëŠ” ì œëª©ì´ ì•„ë‹˜ (ì‹¤ì œ ì§ˆë¬¸ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
    if re.match(r'^\d+\.\s+\*\*.*\*\*', text):
        return False
        
    # 3. ì§ˆë¬¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ëŠ” ì œëª©ì´ ì•„ë‹˜
    question_keywords = ['ì„¤ëª…í•´ì£¼ì„¸ìš”', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ', 'ë°©ë²•', 'ì°¨ì´ì ', 'ì¥ì ', 'ë‹¨ì ', 
                        'ì˜ˆì‹œ', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ë¹„êµ', 'ì„ íƒ', 'ê³ ë ¤', 'ì ìš©', 'ì‚¬ìš©', '?']
    if any(keyword in text for keyword in question_keywords):
        return False
    
    # 4. ì œëª© í˜•íƒœ íŒ¨í„´ (ì‹¤ì œ ì„¹ì…˜ ì œëª©ë“¤ë§Œ)
    title_patterns = [
        r'^[ê°€-í£\s]*ê¸°ìˆ \s*ë©´ì ‘\s*ì§ˆë¬¸[ê°€-í£\s]*$',  # "ê¸°ìˆ  ë©´ì ‘ ì§ˆë¬¸"ìœ¼ë¡œë§Œ êµ¬ì„±
        r'^[ê°€-í£\s]*ì•„í‚¤í…ì²˜[ê°€-í£\s]*$',           # "ì•„í‚¤í…ì²˜"ë§Œ í¬í•¨í•˜ëŠ” ë‹¨ìˆœ ì œëª©
        r'^[ê°€-í£\s]*ê´€ë ¨\s*ì§ˆë¬¸[ê°€-í£\s]*$',        # "ê´€ë ¨ ì§ˆë¬¸"ìœ¼ë¡œë§Œ êµ¬ì„±
        r'^[ê°€-í£\s]*ë©´ì ‘\s*ë¬¸ì œ[ê°€-í£\s]*$',        # "ë©´ì ‘ ë¬¸ì œ"ë¡œë§Œ êµ¬ì„±
    ]
    
    for pattern in title_patterns:
        if re.match(pattern, text, re.IGNORECASE):
            return True
    
    # 5. ì§ˆë¬¸ì´ ì•„ë‹Œ ì§§ì€ ë¬¸ì¥ (ë¬¼ìŒí‘œê°€ ì—†ê³  ë„ˆë¬´ ì§§ì€ ê²½ìš°)
    if (len(text) < 15 and '?' not in text and 
        not any(keyword in text for keyword in ['ì„¤ëª…', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ'])):
        return True
    
    # 6. ë§ˆí¬ë‹¤ìš´ êµ¬ë¶„ì
    if text in ['---', '***', '===']:
        return True
    
    return False


def is_valid_question(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ ìœ íš¨í•œ ì§ˆë¬¸ì¸ì§€ í™•ì¸
    """
    text = text.strip()
    
    # 1. ìµœì†Œ ê¸¸ì´ í™•ì¸
    if len(text) < 10:
        return False
    
    # 2. ì§ˆë¬¸ í‚¤ì›Œë“œ í™•ì¸
    question_indicators = [
        '?', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ', 'ì„¤ëª…', 'ì°¨ì´ì ', 'ì¥ì ', 'ë‹¨ì ', 
        'ë°©ë²•', 'ì „ëµ', 'êµ¬í˜„', 'ì‚¬ìš©', 'ì ìš©', 'ê³ ë ¤', 'ì²˜ë¦¬', 'í•´ê²°'
    ]
    
    has_question_indicator = any(indicator in text for indicator in question_indicators)
    
    # 3. ì œëª©/í—¤ë”ê°€ ì•„ë‹Œì§€ í™•ì¸
    is_not_header = not is_header_or_title(text)
    
    return has_question_indicator and is_not_header


def parse_compound_question(question: QuestionResponse) -> List[QuestionResponse]:
    """
    ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì„ ì •ë¦¬í•˜ì—¬ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
    
    Args:
        question: ì›ë³¸ ì§ˆë¬¸ ê°ì²´
        
    Returns:
        List[QuestionResponse]: ì •ë¦¬ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    question_text = question.question
    
    # 1. ë§ˆí¬ë‹¤ìš´ ì œëª©ê³¼ ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
    question_text = re.sub(r'^#{1,6}\s+.*$', '', question_text, flags=re.MULTILINE)  # ë§ˆí¬ë‹¤ìš´ ì œëª© ì œê±°
    question_text = re.sub(r'^---+\s*$', '', question_text, flags=re.MULTILINE)     # êµ¬ë¶„ì ì œê±°
    question_text = re.sub(r'\n\s*\n', '\n\n', question_text)                      # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ ì •ë¦¬
    
    # 2. ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
    lines = question_text.split('\n')
    processed_lines = []
    
    for line in lines:
        line_stripped = line.strip()
        if not line_stripped:
            continue
            
        # ë§ˆí¬ë‹¤ìš´ ì œëª© ìŠ¤í‚µ
        if re.match(r'^#{1,6}\s+', line_stripped):
            continue
            
        # numbered list í•­ëª©ì˜ ë²ˆí˜¸ ì œê±° ë° ì •ë¦¬
        clean_line = line_stripped
        
        # ë‹¤ì–‘í•œ numbered list íŒ¨í„´ ì²˜ë¦¬
        patterns_to_remove = [
            r'^\d+\.\s+',      # "1. "
            r'^\d+\)\s+',      # "1) "
            r'^\s*\d+\.\s+',  # "  1. "
        ]
        
        for pattern in patterns_to_remove:
            if re.match(pattern, clean_line):
                clean_line = re.sub(pattern, '', clean_line).strip()
                break
        
        # ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
        if clean_line:
            processed_lines.append(clean_line)
    
    # 3. ì²˜ë¦¬ëœ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì§ˆë¬¸ìœ¼ë¡œ ê²°í•©
    cleaned_question = ' '.join(processed_lines).strip()
    
    # 4. ì •ë¦¬ëœ ì§ˆë¬¸ì´ ìœ íš¨í•œì§€ í™•ì¸
    if (len(cleaned_question) > 20 and 
        any(keyword in cleaned_question for keyword in ['ì„¤ëª…í•´ì£¼ì„¸ìš”', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ', 'ë°©ë²•', 'ì°¨ì´ì ', '?', 'ì˜ˆì‹œ', 'êµ¬ì²´ì '])):
        
        # ì •ë¦¬ëœ ì§ˆë¬¸ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        question.question = cleaned_question
        return [question]
    
    # 5. ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return [question]


def parse_questions_list(questions: List[QuestionResponse]) -> List[QuestionResponse]:
    """
    ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ compound questionë“¤ì„ ë¶„ë¦¬
    
    Args:
        questions: ì›ë³¸ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[QuestionResponse]: íŒŒì‹±ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    parsed_questions = []
    
    for question in questions:
        # ê° ì§ˆë¬¸ì„ íŒŒì‹±í•˜ì—¬ ê²°ê³¼ ì¶”ê°€
        parsed_list = parse_compound_question(question)
        for parsed_question in parsed_list:
            parsed_questions.append(normalize_question_response(parsed_question))
    
    return parsed_questions


class QuestionGenerationResult(BaseModel):
    """ì§ˆë¬¸ ìƒì„± ê²°ê³¼"""
    success: bool
    questions: List[QuestionResponse]
    analysis_id: Optional[str] = None
    error: Optional[str] = None


@router.post("/generate", response_model=QuestionGenerationResult)
async def generate_questions(
    request: QuestionGenerationRequest,
    github_token: Optional[str] = Header(None, alias="x-github-token"),
    google_api_key: Optional[str] = Header(None, alias="x-google-api-key")
):
    """GitHub ì €ì¥ì†Œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±"""
    
    try:
        # ë¶„ì„ ê²°ê³¼ì—ì„œ analysis_id ì¶”ì¶œ
        analysis_id = None
        if request.analysis_result and "analysis_id" in request.analysis_result:
            analysis_id = request.analysis_result["analysis_id"]

        # ğŸ”§ í•µì‹¬ ìˆ˜ì •: ì¤‘ë³µ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•œ Redis ë½ ì‚¬ìš©
        if analysis_id and not request.force_regenerate:
            from app.core.database import get_redis
            import asyncio
            
            lock_key = f"question_generation_lock:{analysis_id}"
            lock_timeout = 300  # 5ë¶„ (ì§ˆë¬¸ ìƒì„±ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
            
            try:
                redis = await get_redis()
                
                # ğŸ”¥ Redis ë½ íšë“ ì‹œë„ (ì´ë¯¸ ìƒì„± ì¤‘ì¸ ìš”ì²­ì´ ìˆìœ¼ë©´ ëŒ€ê¸° ë˜ëŠ” ê±°ë¶€)
                lock_acquired = await redis.set(lock_key, "generating", ex=lock_timeout, nx=True)
                
                if not lock_acquired:
                    # ë½ì„ íšë“í•˜ì§€ ëª»í•œ ê²½ìš°: ë‹¤ë¥¸ ìš”ì²­ì´ ì´ë¯¸ ì§„í–‰ ì¤‘
                    print(f"[LOCK_BLOCKED] ì§ˆë¬¸ ìƒì„±ì´ ì´ë¯¸ ì§„í–‰ ì¤‘: analysis_id={analysis_id}")
                    
                    # ì ì‹œ ëŒ€ê¸° í›„ ìºì‹œì—ì„œ ê²°ê³¼ í™•ì¸
                    for attempt in range(10):  # ìµœëŒ€ 10íšŒ ì‹œë„ (ì•½ 50ì´ˆ)
                        await asyncio.sleep(5)
                        
                        # ì •ê·œí™”ëœ ìºì‹œ í‚¤ë¡œ í™•ì¸
                        normalized_cache_key = analysis_id.replace('-', '')
                        if normalized_cache_key in question_cache:
                            cache_data = question_cache[normalized_cache_key]
                            print(f"[LOCK_WAIT_SUCCESS] ëŒ€ê¸° ì¤‘ ì§ˆë¬¸ ìƒì„± ì™„ë£Œë¨: {len(cache_data.parsed_questions)}ê°œ ì§ˆë¬¸")
                            return QuestionGenerationResult(
                                success=True,
                                questions=cache_data.parsed_questions,
                                analysis_id=analysis_id
                            )
                        
                        # í•˜ì´í”ˆ í¬í•¨ í‚¤ë¡œë„ í™•ì¸
                        if analysis_id in question_cache:
                            cache_data = question_cache[analysis_id]
                            print(f"[LOCK_WAIT_SUCCESS] ëŒ€ê¸° ì¤‘ ì§ˆë¬¸ ìƒì„± ì™„ë£Œë¨ (í•˜ì´í”ˆ í‚¤): {len(cache_data.parsed_questions)}ê°œ ì§ˆë¬¸")
                            return QuestionGenerationResult(
                                success=True,
                                questions=cache_data.parsed_questions,
                                analysis_id=analysis_id
                            )
                    
                    # ëŒ€ê¸° ì‹œê°„ì´ ì´ˆê³¼ëœ ê²½ìš°
                    raise HTTPException(
                        status_code=409,
                        detail={
                            "error": "GENERATION_IN_PROGRESS",
                            "message": "ì§ˆë¬¸ ìƒì„±ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                            "analysis_id": analysis_id,
                            "suggestion": "ì§ˆë¬¸ ëª©ë¡ ì¡°íšŒë¥¼ í†µí•´ ìƒì„± ì™„ë£Œ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                        }
                    )
                
                # ë½ íšë“ ì„±ê³µ - ì§ˆë¬¸ ìƒì„± ì§„í–‰
                print(f"[LOCK_ACQUIRED] ì§ˆë¬¸ ìƒì„± ë½ íšë“ ì„±ê³µ: analysis_id={analysis_id}")
                
            except HTTPException:
                raise
            except Exception as e:
                print(f"[LOCK_ERROR] Redis ë½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                # Redis ì—°ê²° ì‹¤íŒ¨ ì‹œì—ë„ ì§ˆë¬¸ ìƒì„±ì€ ê³„ì† ì§„í–‰
        
        # ì´ë¯¸ ìƒì„±ëœ ì§ˆë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸ (ê°•ì œ ì¬ìƒì„±ì´ ì•„ë‹Œ ê²½ìš°)
        if analysis_id and not request.force_regenerate:
            # ì •ê·œí™”ëœ í‚¤ë¡œ ë¨¼ì € í™•ì¸
            normalized_cache_key = analysis_id.replace('-', '')
            if normalized_cache_key in question_cache:
                cache_data = question_cache[normalized_cache_key]
                print(f"[CACHE_HIT] ê¸°ì¡´ ì§ˆë¬¸ ë°˜í™˜ (ì •ê·œí™” í‚¤): {len(cache_data.parsed_questions)}ê°œ")
                return QuestionGenerationResult(
                    success=True,
                    questions=cache_data.parsed_questions,
                    analysis_id=analysis_id
                )
            
            # í•˜ì´í”ˆ í¬í•¨ í‚¤ë¡œë„ í™•ì¸
            if analysis_id in question_cache:
                cache_data = question_cache[analysis_id]
                print(f"[CACHE_HIT] ê¸°ì¡´ ì§ˆë¬¸ ë°˜í™˜ (í•˜ì´í”ˆ í‚¤): {len(cache_data.parsed_questions)}ê°œ")
                return QuestionGenerationResult(
                    success=True,
                    questions=cache_data.parsed_questions,
                    analysis_id=analysis_id
                )
        
        # í—¤ë”ì—ì„œ API í‚¤ ì¶”ì¶œ
        from app.core.api_utils import extract_api_keys_from_headers
        api_keys = extract_api_keys_from_headers(github_token, google_api_key)
        
        # ì§ˆë¬¸ ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = QuestionGenerator()
        
        # ì§ˆë¬¸ ìƒì„± ì‹¤í–‰ - QuestionGenerator ë‚´ë¶€ ê¸°ë³¸ê°’ ì‚¬ìš© (3ê°€ì§€ íƒ€ì… ê· ë“± ë¶„ë°°)
        result = await generator.generate_questions(
            repo_url=request.repo_url,
            difficulty_level=request.difficulty,
            question_count=request.question_count,
            question_types=None,  # ê¸°ë³¸ê°’ ["tech_stack", "architecture", "code_analysis"] ì‚¬ìš©
            analysis_data=request.analysis_result,
            api_keys=api_keys  # API í‚¤ ì „ë‹¬
        )
        
        if not result["success"]:
            raise HTTPException(status_code=500, detail=result.get("error", "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨"))
        
        # ì‘ë‹µ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
        questions = []
        for q in result["questions"]:
            questions.append(QuestionResponse(
                id=q.get("id", ""),
                type=q.get("type", "technical"),
                question=q.get("question", ""),
                difficulty=q.get("difficulty", request.difficulty),
                context=q.get("context"),
                time_estimate=q.get("time_estimate", "5ë¶„"),
                code_snippet=q.get("code_snippet"),
                expected_answer_points=q.get("expected_answer_points"),
                technology=q.get("technology"),
                pattern=q.get("pattern")
            ))
        
        # ì§ˆë¬¸ íŒŒì‹± ì²˜ë¦¬ (compound question ë¶„ë¦¬)
        parsed_questions = parse_questions_list(questions)
        
        # ì§ˆë¬¸ ê·¸ë£¹ ê´€ê³„ ìƒì„±
        question_groups = create_question_groups(parsed_questions)
        
        # ìºì‹œì— ì €ì¥ (êµ¬ì¡°í™”ëœ ë°ì´í„°) - UUID ì •ê·œí™”í•˜ì—¬ ì €ì¥
        if analysis_id:
            from datetime import datetime
            # UUID ì •ê·œí™”: í•˜ì´í”ˆ ì œê±°í•˜ì—¬ ì¼ê´€ì„± ìˆê²Œ ì €ì¥
            normalized_cache_key = analysis_id.replace('-', '')
            cache_data = QuestionCacheData(
                original_questions=questions,
                parsed_questions=parsed_questions,
                question_groups=question_groups,
                created_at=datetime.now().isoformat()
            )
            question_cache[normalized_cache_key] = cache_data
            print(f"[CACHE] ì§ˆë¬¸ì„ ìºì‹œì— ì €ì¥: ì›ë³¸í‚¤={analysis_id}, ì •ê·œí™”í‚¤={normalized_cache_key}, ì§ˆë¬¸ìˆ˜={len(parsed_questions)}")
            
            # í•˜ì´í”ˆ ìˆëŠ” í‚¤ë¡œë„ ì €ì¥ (í˜¸í™˜ì„± ë³´ì¥)
            question_cache[analysis_id] = cache_data
            
            # DBì—ë„ ì €ì¥í•˜ì—¬ ì˜êµ¬ ë³´ì¡´
            await _save_questions_to_db(analysis_id, parsed_questions)
        
        # ğŸ”§ Redis ë½ í•´ì œ (ì§ˆë¬¸ ìƒì„± ì™„ë£Œ)
        if analysis_id:
            try:
                from app.core.database import get_redis
                redis = await get_redis()
                lock_key = f"question_generation_lock:{analysis_id}"
                await redis.delete(lock_key)
                print(f"[LOCK_RELEASED] ì§ˆë¬¸ ìƒì„± ë½ í•´ì œ ì™„ë£Œ: analysis_id={analysis_id}")
            except Exception as lock_error:
                print(f"[LOCK_ERROR] Redis ë½ í•´ì œ ì‹¤íŒ¨ (ì§ˆë¬¸ ìƒì„±ì€ ì„±ê³µ): {str(lock_error)}")
        
        return QuestionGenerationResult(
            success=True,
            questions=parsed_questions,
            analysis_id=analysis_id
        )
        
    except HTTPException:
        raise
    except Exception as e:
        # ğŸ”§ ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ Redis ë½ í•´ì œ
        if analysis_id:
            try:
                from app.core.database import get_redis
                redis = await get_redis()
                lock_key = f"question_generation_lock:{analysis_id}"
                await redis.delete(lock_key)
                print(f"[LOCK_RELEASED] ì˜ˆì™¸ ë°œìƒìœ¼ë¡œ ì¸í•œ ë½ í•´ì œ: analysis_id={analysis_id}")
            except Exception as lock_error:
                print(f"[LOCK_ERROR] ì˜ˆì™¸ ìƒí™©ì—ì„œ ë½ í•´ì œ ì‹¤íŒ¨: {str(lock_error)}")
                
        return QuestionGenerationResult(
            success=False,
            questions=[],
            error=str(e)
        )


@router.get("/types")
async def get_question_types():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì§ˆë¬¸ íƒ€ì… ëª©ë¡ ì¡°íšŒ"""
    return {
        "question_types": [
            "code_analysis",
            "tech_stack",
            "architecture",
            "design_patterns",
            "problem_solving",
            "best_practices"
        ],
        "difficulties": ["easy", "medium", "hard"]
    }


@router.get("/{analysis_id}")
async def get_questions(analysis_id: str):
    """ë¶„ì„ IDë¡œ ì§ˆë¬¸ ì¡°íšŒ"""
    try:
        # UUID ì •ê·œí™”: í•˜ì´í”ˆ ì œê±°í•˜ì—¬ ìºì‹œ í‚¤ì™€ ë§¤ì¹­
        normalized_analysis_id = analysis_id.replace('-', '')
        
        if normalized_analysis_id not in question_cache:
            raise HTTPException(status_code=404, detail="ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        cache_data = question_cache[normalized_analysis_id]
        normalized_questions = [
            normalize_question_response(q)
            for q in cache_data.parsed_questions
        ]
        return {
            "success": True,
            "questions": normalized_questions,
            "question_groups": cache_data.question_groups,
            "created_at": cache_data.created_at
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{analysis_id}/groups")
async def get_question_groups(analysis_id: str):
    """ì§ˆë¬¸ ê·¸ë£¹ ì •ë³´ ì¡°íšŒ"""
    try:
        # UUID ì •ê·œí™”: í•˜ì´í”ˆ ì œê±°í•˜ì—¬ ìºì‹œ í‚¤ì™€ ë§¤ì¹­
        normalized_analysis_id = analysis_id.replace('-', '')
        
        if normalized_analysis_id not in question_cache:
            raise HTTPException(status_code=404, detail="ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        cache_data = question_cache[normalized_analysis_id]
        return {
            "success": True,
            "question_groups": cache_data.question_groups,
            "total_questions": len(cache_data.parsed_questions),
            "total_groups": len(cache_data.question_groups)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/cache/clear")
async def clear_question_cache():
    """ì§ˆë¬¸ ìºì‹œ ì´ˆê¸°í™”"""
    try:
        global question_cache
        cache_count = len(question_cache)
        question_cache.clear()
        
        return {
            "success": True,
            "message": f"ì§ˆë¬¸ ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ({cache_count}ê°œ í•­ëª© ì œê±°)",
            "cleared_count": cache_count
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/cache/status")
async def get_cache_status():
    """ì§ˆë¬¸ ìºì‹œ ìƒíƒœ ì¡°íšŒ"""
    try:
        cache_info = {}
        for analysis_id, cache_data in question_cache.items():
            cache_info[analysis_id] = {
                "original_questions_count": len(cache_data.original_questions),
                "parsed_questions_count": len(cache_data.parsed_questions),
                "groups_count": len(cache_data.question_groups),
                "created_at": cache_data.created_at
            }
        
        return {
            "success": True,
            "total_cached_analyses": len(question_cache),
            "cache_details": cache_info
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/analysis/{analysis_id}")
async def get_questions_by_analysis(analysis_id: str):
    """ë¶„ì„ IDë¡œ ìƒì„±ëœ ì§ˆë¬¸ ì¡°íšŒ - ë©”ëª¨ë¦¬ ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ DB ì¡°íšŒ"""
    try:
        # 1. ë¨¼ì € ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ì¡°íšŒ (UUID ì •ê·œí™”)
        normalized_analysis_id = analysis_id.replace('-', '')
        if normalized_analysis_id in question_cache:
            print(f"[QUESTIONS] Found questions in memory cache for {analysis_id} (normalized: {normalized_analysis_id})")
            cache_data = question_cache[normalized_analysis_id]
            
            # ìºì‹œ ë°ì´í„° êµ¬ì¡° í™•ì¸
            questions = []
            if hasattr(cache_data, 'parsed_questions'):
                questions = cache_data.parsed_questions
            elif hasattr(cache_data, 'questions'):
                questions = cache_data.questions
            elif isinstance(cache_data, list):
                questions = cache_data
            
            normalized_questions = [
                normalize_question_response(q)
                for q in questions
            ]

            return QuestionGenerationResult(
                success=True,
                questions=normalized_questions,
                analysis_id=analysis_id
            )
        
        # 2. ë©”ëª¨ë¦¬ ìºì‹œì— ì—†ìœ¼ë©´ DBì—ì„œ ì¡°íšŒ
        print(f"[QUESTIONS] Memory cache miss, checking database for {analysis_id}")
        db_questions = await _load_questions_from_db(analysis_id)
        
        if db_questions:
            print(f"[QUESTIONS] Found {len(db_questions)} questions in database, restoring to cache")
            
            # DBì—ì„œ ê°€ì ¸ì˜¨ ì§ˆë¬¸ë“¤ì„ ë©”ëª¨ë¦¬ ìºì‹œì— ë³µì›
            await _restore_questions_to_cache(analysis_id, db_questions)
            
            return QuestionGenerationResult(
                success=True,
                questions=db_questions,
                analysis_id=analysis_id
            )
        
        # 3. ë©”ëª¨ë¦¬ ìºì‹œì™€ DB ëª¨ë‘ì— ì—†ìŒ
        print(f"[QUESTIONS] No questions found for {analysis_id} in cache or database")
        return QuestionGenerationResult(
            success=False,
            questions=[],
            analysis_id=analysis_id,
            error="í•´ë‹¹ ë¶„ì„ IDì— ëŒ€í•œ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."
        )
        
    except Exception as e:
        print(f"Error in get_questions_by_analysis: {e}")
        return QuestionGenerationResult(
            success=False,
            questions=[],
            analysis_id=analysis_id,
            error=f"ì§ˆë¬¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/debug/cache")
async def debug_question_cache():
    """ì§ˆë¬¸ ìºì‹œ ìƒíƒœ í™•ì¸ (ë””ë²„ê¹…ìš©)"""
    if not settings.debug:
        raise HTTPException(status_code=404, detail="Not found")
    return {
        "cache_size": len(question_cache),
        "cached_analysis_ids": list(question_cache.keys()),
        "cache_details": [
            {
                "analysis_id": analysis_id,
                "original_question_count": len(cache_data.original_questions) if hasattr(cache_data, 'original_questions') else 0,
                "parsed_question_count": len(cache_data.parsed_questions) if hasattr(cache_data, 'parsed_questions') else 0,
                "question_types": list(set(q.type for q in cache_data.parsed_questions)) if hasattr(cache_data, 'parsed_questions') else []
            }
            for analysis_id, cache_data in question_cache.items()
        ]
    }


@router.get("/debug/original/{analysis_id}")
async def debug_original_questions(analysis_id: str):
    """ì›ë³¸ ì§ˆë¬¸ í™•ì¸ (ë””ë²„ê¹…ìš©)"""
    if not settings.debug:
        raise HTTPException(status_code=404, detail="Not found")
    try:
        # UUID ì •ê·œí™”: í•˜ì´í”ˆ ì œê±°í•˜ì—¬ ìºì‹œ í‚¤ì™€ ë§¤ì¹­
        normalized_analysis_id = analysis_id.replace('-', '')
        
        if normalized_analysis_id not in question_cache:
            raise HTTPException(status_code=404, detail="ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        cache_data = question_cache[normalized_analysis_id]
        return {
            "success": True,
            "original_questions": [
                {
                    "id": q.id,
                    "type": q.type, 
                    "question": q.question,
                    "is_compound": q.is_compound_question,
                    "total_sub_questions": q.total_sub_questions
                }
                for q in cache_data.original_questions
            ],
            "parsed_questions_count": len(cache_data.parsed_questions),
            "groups_count": len(cache_data.question_groups)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/debug/add-test-questions/{analysis_id}")
async def add_test_questions(analysis_id: str):
    """í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ ì¶”ê°€ (ë””ë²„ê¹…ìš©)"""
    if not settings.debug:
        raise HTTPException(status_code=404, detail="Not found")
    try:
        from datetime import datetime
        
        # í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ ìƒì„±
        test_questions = [
            QuestionResponse(
                id=str(uuid.uuid4()),
                type="technical",
                question="Linux ì»¤ë„ì˜ ì£¼ìš” ì„œë¸Œì‹œìŠ¤í…œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                difficulty="medium",
                context="Linux ì»¤ë„ ì•„í‚¤í…ì²˜",
                time_estimate="5ë¶„",
                technology="C"
            ),
            QuestionResponse(
                id=str(uuid.uuid4()),
                type="architecture",
                question="1. ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„œë¸Œì‹œìŠ¤í…œì˜ ì—­í• ì€?\n2. í”„ë¡œì„¸ìŠ¤ ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ë™ì‘ ì›ë¦¬ëŠ”?\n3. íŒŒì¼ ì‹œìŠ¤í…œì˜ VFS ë ˆì´ì–´ ëª©ì ì€?",
                difficulty="medium",
                context="Linux ì»¤ë„ ì•„í‚¤í…ì²˜",
                time_estimate="10ë¶„",
                technology="C"
            ),
            QuestionResponse(
                id=str(uuid.uuid4()),
                type="code_analysis",
                question="ë””ë°”ì´ìŠ¤ ë“œë¼ì´ë²„ë¥¼ ì‘ì„±í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ì£¼ìš” ìš”ì†Œë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                difficulty="medium",
                context="Linux ì»¤ë„ ê°œë°œ",
                time_estimate="7ë¶„",
                technology="C"
            )
        ]
        
        # ì§ˆë¬¸ íŒŒì‹± ì²˜ë¦¬
        parsed_questions = parse_questions_list(test_questions)
        
        # ì§ˆë¬¸ ê·¸ë£¹ ê´€ê³„ ìƒì„±
        question_groups = create_question_groups(parsed_questions)
        
        # ìºì‹œì— ì €ì¥ (UUID ì •ê·œí™”)
        normalized_cache_key = analysis_id.replace('-', '')
        cache_data = QuestionCacheData(
            original_questions=test_questions,
            parsed_questions=parsed_questions,
            question_groups=question_groups,
            created_at=datetime.now().isoformat()
        )
        question_cache[normalized_cache_key] = cache_data
        # í˜¸í™˜ì„±ì„ ìœ„í•´ ì›ë³¸ í‚¤ë¡œë„ ì €ì¥
        question_cache[analysis_id] = cache_data
        
        return {
            "success": True,
            "message": f"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. (ì›ë³¸: {len(test_questions)}, íŒŒì‹±: {len(parsed_questions)})",
            "analysis_id": analysis_id,
            "questions": parsed_questions
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


async def _load_questions_from_db(analysis_id: str) -> List[QuestionResponse]:
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì§ˆë¬¸ ì¡°íšŒ"""
    try:
        with engine.connect() as conn:
            # InterviewQuestion í…Œì´ë¸”ì—ì„œ ì§ˆë¬¸ ì¡°íšŒ
            result = conn.execute(text(
                """
                SELECT id, category, difficulty, question_text, expected_points, 
                       related_files, context, created_at
                FROM interview_questions 
                WHERE analysis_id = :analysis_id
                ORDER BY created_at ASC
                """
            ), {"analysis_id": analysis_id})
            
            questions = []
            for row in result:
                # expected_points JSON íŒŒì‹±
                expected_points = None
                if row[4]:  # expected_points í•„ë“œ
                    try:
                        expected_points = json.loads(row[4]) if isinstance(row[4], str) else row[4]
                    except json.JSONDecodeError:
                        expected_points = None
                
                # ë°ì´í„°ë² ì´ìŠ¤ rowë¥¼ QuestionResponse ê°ì²´ë¡œ ë³€í™˜
                question = QuestionResponse(
                    id=str(row[0]),
                    type=row[1],  # category -> type
                    question=row[3],  # question_text -> question
                    difficulty=row[2],
                    context=None,  # contextëŠ” JSONì´ë¯€ë¡œ ê°„ë‹¨íˆ Noneìœ¼ë¡œ ì²˜ë¦¬
                    time_estimate="5ë¶„",  # ê¸°ë³¸ê°’
                    code_snippet=None,
                    expected_answer_points=expected_points,
                    technology=None,
                    pattern=None
                )
                questions.append(normalize_question_response(question))
            
            print(f"[DB] Loaded {len(questions)} questions from database for analysis {analysis_id}")
            return questions
            
    except Exception as e:
        print(f"[DB] Error loading questions from database: {e}")
        return []


async def _restore_questions_to_cache(analysis_id: str, questions: List[QuestionResponse]):
    """DBì—ì„œ ê°€ì ¸ì˜¨ ì§ˆë¬¸ë“¤ì„ ë©”ëª¨ë¦¬ ìºì‹œì— ë³µì›"""
    try:
        from datetime import datetime
        
        normalized_questions = [normalize_question_response(q) for q in questions]

        # ì§ˆë¬¸ ê·¸ë£¹ ê´€ê³„ ìƒì„±
        question_groups = create_question_groups(normalized_questions)
        
        # ìºì‹œì— ì €ì¥í•  ë°ì´í„° êµ¬ì¡° ìƒì„±
        cache_data = QuestionCacheData(
            original_questions=normalized_questions,  # DBì—ì„œ ê°€ì ¸ì˜¨ ì§ˆë¬¸ë“¤ì„ ì›ë³¸ìœ¼ë¡œ ì²˜ë¦¬
            parsed_questions=normalized_questions,    # ì´ë¯¸ íŒŒì‹±ëœ ìƒíƒœë¡œ ê°„ì£¼
            question_groups=question_groups,
            created_at=datetime.now().isoformat()
        )
        
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥ (UUID ì •ê·œí™”í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€)
        normalized_cache_key = analysis_id.replace('-', '')
        question_cache[normalized_cache_key] = cache_data
        # í˜¸í™˜ì„±ì„ ìœ„í•´ ì›ë³¸ í‚¤ë¡œë„ ì €ì¥
        question_cache[analysis_id] = cache_data
        
        print(f"[CACHE] Restored {len(normalized_questions)} questions to memory cache for analysis {analysis_id} (normalized: {normalized_cache_key})")
        
    except Exception as e:
        print(f"[CACHE] Error restoring questions to cache: {e}")


async def _save_questions_to_db(analysis_id: str, questions: List[QuestionResponse]):
    """ìƒì„±ëœ ì§ˆë¬¸ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ - UPSERT ë°©ì‹ìœ¼ë¡œ ê°œì„ """
    try:
        from app.core.database import database_url
        from datetime import datetime
        current_time = datetime.now()
        
        with engine.connect() as conn:
            # ğŸ”§ í•µì‹¬ ê°œì„ : DELETE-INSERT ëŒ€ì‹  UPSERT ì‚¬ìš©
            # ë°ì´í„°ë² ì´ìŠ¤ ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ë¥¸ UPSERT êµ¬ë¬¸ ì‚¬ìš©
            is_sqlite = "sqlite" in database_url.lower()
            
            print(f"[DB_UPSERT] UPSERT ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸ ì €ì¥ ì‹œì‘: {len(questions)}ê°œ ì§ˆë¬¸, DBíƒ€ì…={'SQLite' if is_sqlite else 'PostgreSQL'}")
            
            # ğŸ”¥ ë‹¨ê³„ë³„ UPSERT: ê¸°ì¡´ ì§ˆë¬¸ ID ì¡°íšŒ í›„ ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
            for question in questions:
                if is_sqlite:
                    # SQLite: INSERT OR REPLACE ì‚¬ìš© (updated_at ì»¬ëŸ¼ ì—†ìŒ)
                    conn.execute(text(
                        """
                        INSERT OR REPLACE INTO interview_questions 
                        (id, analysis_id, category, difficulty, question_text, expected_points, created_at)
                        VALUES (:id, :analysis_id, :category, :difficulty, :question_text, :expected_points, 
                                COALESCE((SELECT created_at FROM interview_questions WHERE id = :id), :created_at))
                        """
                    ), {
                        "id": question.id,
                        "analysis_id": analysis_id,
                        "category": question.type,
                        "difficulty": question.difficulty,
                        "question_text": question.question,
                        "expected_points": json.dumps(question.expected_answer_points) if question.expected_answer_points else None,
                        "created_at": current_time
                    })
                else:
                    # PostgreSQL: INSERT ... ON CONFLICT DO UPDATE ì‚¬ìš© (updated_at ì»¬ëŸ¼ ì—†ìŒ)
                    conn.execute(text(
                        """
                        INSERT INTO interview_questions 
                        (id, analysis_id, category, difficulty, question_text, expected_points, created_at)
                        VALUES (:id, :analysis_id, :category, :difficulty, :question_text, :expected_points, :created_at)
                        ON CONFLICT (id) DO UPDATE SET
                            category = EXCLUDED.category,
                            difficulty = EXCLUDED.difficulty,
                            question_text = EXCLUDED.question_text,
                            expected_points = EXCLUDED.expected_points
                        """
                    ), {
                        "id": question.id,
                        "analysis_id": analysis_id,
                        "category": question.type,
                        "difficulty": question.difficulty,
                        "question_text": question.question,
                        "expected_points": json.dumps(question.expected_answer_points) if question.expected_answer_points else None,
                        "created_at": current_time
                    })
            
            # ğŸ”§ ì¶”ê°€ ê°œì„ : í˜„ì¬ ì§ˆë¬¸ ì„¸íŠ¸ì— ì—†ëŠ” ê¸°ì¡´ ì§ˆë¬¸ë“¤ì€ ë¹„í™œì„±í™” (ì‚­ì œí•˜ì§€ ì•ŠìŒ)
            current_question_ids = [q.id for q in questions]
            if current_question_ids:
                question_ids_placeholder = ','.join([f"'{qid}'" for qid in current_question_ids])
                
                # í˜„ì¬ ì§ˆë¬¸ ì„¸íŠ¸ì— ì—†ëŠ” ê¸°ì¡´ ì§ˆë¬¸ë“¤ ë¹„í™œì„±í™”
                result = conn.execute(text(
                    f"""
                    UPDATE interview_questions 
                    SET is_active = FALSE, updated_at = :updated_at
                    WHERE analysis_id = :analysis_id 
                    AND id NOT IN ({question_ids_placeholder})
                    AND is_active = TRUE
                    """
                ), {
                    "analysis_id": analysis_id,
                    "updated_at": current_time
                })
                
                deactivated_count = result.rowcount if hasattr(result, 'rowcount') else 0
                if deactivated_count > 0:
                    print(f"[DB_UPSERT] ê¸°ì¡´ ì§ˆë¬¸ {deactivated_count}ê°œ ë¹„í™œì„±í™” (ì‚­ì œí•˜ì§€ ì•ŠìŒ)")
            
            # ë³€ê²½ì‚¬í•­ ì»¤ë°‹
            conn.commit()
            
            print(f"[DB_UPSERT] UPSERT ì™„ë£Œ: {len(questions)}ê°œ ì§ˆë¬¸ ì €ì¥/ì—…ë°ì´íŠ¸, analysis_id={analysis_id}")
            print(f"[DB_UPSERT] âœ… ì¥ì : ì§ˆë¬¸ ì‚­ì œ ì—†ì´ ì›ìì  ì—…ë°ì´íŠ¸ë¡œ ìºì‹œ-DB ì¼ê´€ì„± ë³´ì¥")
            
    except Exception as e:
        print(f"[DB_UPSERT_ERROR] UPSERT ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        print(f"[DB_UPSERT_ERROR] í´ë°±: ê¸°ì¡´ DELETE-INSERT ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
        
        # ğŸ”§ í´ë°±: UPSERT ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
        try:
            with engine.connect() as conn:
                # ê¸°ì¡´ ì§ˆë¬¸ ì‚­ì œ
                conn.execute(text(
                    "DELETE FROM interview_questions WHERE analysis_id = :analysis_id"
                ), {"analysis_id": analysis_id})
                
                # ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤ ì €ì¥
                from datetime import datetime
                current_time = datetime.now()
                
                for question in questions:
                    conn.execute(text(
                        """
                        INSERT INTO interview_questions 
                        (id, analysis_id, category, difficulty, question_text, expected_points, created_at)
                        VALUES (:id, :analysis_id, :category, :difficulty, :question_text, :expected_points, :created_at)
                        """
                    ), {
                        "id": question.id,
                        "analysis_id": analysis_id,
                        "category": question.type,
                        "difficulty": question.difficulty,
                        "question_text": question.question,
                        "expected_points": json.dumps(question.expected_answer_points) if question.expected_answer_points else None,
                        "created_at": current_time
                    })
                
                conn.commit()
                print(f"[DB_FALLBACK] í´ë°± ì €ì¥ ì„±ê³µ: {len(questions)}ê°œ ì§ˆë¬¸")
                
        except Exception as fallback_error:
            print(f"[DB_FALLBACK_ERROR] í´ë°±ë„ ì‹¤íŒ¨: {str(fallback_error)}")
            # DB ì €ì¥ ì‹¤íŒ¨ëŠ” ì§ˆë¬¸ ìƒì„± ìì²´ë¥¼ ì‹¤íŒ¨ì‹œí‚¤ì§€ ì•ŠìŒ
